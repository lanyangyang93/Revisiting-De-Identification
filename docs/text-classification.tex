\documentclass{article}
\usepackage{arxiv}
\usepackage[utf8]{inputenc}

\usepackage{natbib}
\usepackage{float}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{hyperref}


\title{Text Classification}
\author{Enwei Zhu}
% \date{}

\begin{document}

\maketitle

\section{English Tasks}
\subsection{Settings}
\begin{itemize}
    \item Word embeddings are initialized with GloVe
    \item From scratch
    \begin{itemize}
        \item Optimizer: Adadelta (lr=0.5)
        \item Batch size: 64
        \item Number of epochs: 50
    \end{itemize}
    \item Fine tuning 
    \begin{itemize}
        \item Optimizer: AdamW (lr=1e-3/2e-3, ft\_lr=1e-5)
        \item Batch size: 32
        \item Number of epochs: 10
        \item Scheduler: Learning rate warmup at the first 20\% steps followed by linear decay
        \item PLMs are loaded with dropout rate of 0.2
    \end{itemize}
\end{itemize}


\subsection{Results}
\subsubsection{IMDb}
\begin{table}[H]
    \centering
    \begin{tabular}{lcccc}
    \toprule
    Model & Paper & Reported Acc. & Our Imp. Acc. & Notes \\
    \midrule
    LSTM + MaxPooling        & --                        & --    & 91.58 & num\_layers=1 \\
    LSTM + Attention         & \citet{mccann2017learned} & 91.1  & 92.09 & num\_layers=1 \\
    BERT-base + Attention    & \citet{sun2019fine}       & 94.60 & 94.37 \\
    RoBERTa-base + Attention & --                        & --    & 95.78 \\
    \bottomrule
    \end{tabular}
    \caption{Results on IMDb.} 
\end{table}


\subsubsection{Yelp Full}
\begin{table}[H]
    \centering
    \begin{tabular}{lcccc}
    \toprule
    Model & Paper & Reported Acc. & Our Imp. Acc. & Notes \\
    \midrule
    LSTM + MaxPooling        & \citet{zhang2015character} & 58.17 & 65.97 & num\_layers=2 \\
    LSTM + Attention         & --                         & --    & 68.61 & num\_layers=2 \\
    BERT-base + Attention    & \citet{sun2019fine}        & 69.94 & 70.27 \\
    RoBERTa-base + Attention & --                         & --    & 71.55 \\
    \bottomrule
    \end{tabular}
    \caption{Results on Yelp Full.} 
\end{table}


\subsubsection{Yelp 2013 (with User and Product IDs)}
\begin{table}[H]
    \centering
    \begin{tabular}{lcccc}
    \toprule
    Model & Paper & Reported Acc. & Our Imp. Acc. & Notes \\
    \midrule
    LSTM + MaxPooling        & \citet{chen2016neural} & 62.7  & 64.96 & num\_layers=2 \\
    LSTM + Attention         & \citet{chen2016neural} & 63.1  & 64.84 & num\_layers=2 \\
    BERT-base + Attention    & --                     & --    & 68.76 \\
    RoBERTa-base + Attention & --                     & --    & 70.80 \\
    \bottomrule
    \end{tabular}
    \caption{Results on Yelp 2013.} 
\end{table}


\newpage
\section{Chinese Tasks}
\subsection{Settings}
\begin{itemize}
    \item Word-based (tokenized by \href{https://github.com/fxsjy/jieba}{jieba})
    \item Word embeddings are initialized by random or with Tencent embeddings \citep{song2018directional}
    \item From scratch
    \begin{itemize}
        \item Optimizer: Adadelta (lr=1.0)
        \item Batch size: 64
        \item Number of epochs: 50
    \end{itemize}
    \item Fine tuning 
    \begin{itemize}
        \item Optimizer: AdamW (lr=1e-3/2e-3, ft\_lr=2e-5)
        \item Batch size: 32
        \item Number of epochs: 10
        \item Scheduler: Learning rate warmup at the first 20\% steps followed by linear decay
        \item PLMs are loaded with dropout rate of 0.2
        \item BERT refers to BERT-wwm \citep{cui2019pretraining}
    \end{itemize}
\end{itemize}


\subsection{Results}
\subsubsection{ChnSentiCorp}
\begin{table}[H]
    \centering
    \begin{tabular}{lcccc}
    \toprule
    Model & Paper & Reported Acc. & Our Imp. Acc. & Notes \\
    \midrule
    Multi-Channel CNN                      & \citet{liu2018exploiting}  & 92.08 \\
    LSTM + MaxPooling                      & --                         & --    & 92.25 & num\_layers=2 \\
    LSTM + Attention                       & --                         & --    & 92.42 & num\_layers=2 \\
    Tencent Embeddings + LSTM + MaxPooling & --                         & --    & 93.50 & num\_layers=2 \\
    Tencent Embeddings + LSTM + Attention  & --                         & --    & 93.08 & num\_layers=2 \\
    BERT-base + Attention                  & \citet{cui2019pretraining} & 95.3  & 95.83 \\
    RoBERTa-base + Attention               & \citet{cui2019pretraining} & 95.8  & 95.08 \\
    \bottomrule
    \end{tabular}
    \caption{Results on ChnSentiCorp.} 
\end{table}

\subsubsection{THUCNews-10}
\begin{table}[H]
    \centering
    \begin{tabular}{lcccc}
    \toprule
    Model & Paper & Reported Acc. & Our Imp. Acc. & Notes \\
    \midrule
    LSTM + MaxPooling                      & --                         & --    & 97.66 & num\_layers=2 \\
    LSTM + Attention                       & --                         & --    & 97.24 & num\_layers=2 \\
    Tencent Embeddings + LSTM + MaxPooling & --                         & --    & 98.79 & num\_layers=2 \\
    Tencent Embeddings + LSTM + Attention  & --                         & --    & 98.57 & num\_layers=2 \\
    BERT-base + Attention                  & \citet{cui2019pretraining} & 97.7  & 98.79 \\
    RoBERTa-base + Attention               & \citet{cui2019pretraining} & 97.8  & 98.98 \\
    \bottomrule
    \end{tabular}
    \caption{Results on THUCNews-10.} 
\end{table}


\newpage
\bibliographystyle{plainnat}
\bibliography{references} 

\end{document}
