\documentclass{article}
\usepackage{arxiv}
\usepackage[utf8]{inputenc}

\usepackage{natbib}
\usepackage{float}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{hyperref}


\title{Relation Extraction}
\author{Enwei Zhu}
% \date{}

\begin{document}

\maketitle

\section{English Tasks}
\subsection{Settings}
\begin{itemize}
    \item Word embeddings are initialized with GloVe
    \item From scratch
    \begin{itemize}
        \item Optimizer: Adadelta (lr=1.0)
        \item Batch size: 64
        \item Number of epochs: 100
    \end{itemize}
    \item Fine tuning 
    \begin{itemize}
        \item Optimizer: AdamW (lr=1e-3/2e-3, ft\_lr=1e-4)
        \item Batch size: 48
        \item Number of epochs: 50
        \item Scheduler: Learning rate warmup at the first 20\% steps followed by linear decay
        \item PLMs are loaded with dropout rate of 0.2
    \end{itemize}
\end{itemize}

\subsection{Results}
\subsubsection{CoNLL 2004}
\begin{table}[H]
    \centering
    \begin{tabular}{lccccc}
    \toprule
    Model & Paper & \makecell{Reported F1 \\(Ent / Rel+)} & \makecell{Our Imp. F1 \\(Ent / Rel+)} & Notes \\
    \midrule
    SpERT (w/ CharLSTM + LSTM)     & --                     & --               & 86.57 / 66.01 & num\_layers=2 \\
    SpERT (w/ BERT-base)           & \citet{eberts2019span} & 88.94$^\dagger$ / 71.47$^\dagger$ & \makecell{88.93 / 70.82 \\(88.80 / 69.78)} \\
    SpERT (w/ BERT-base + LSTM)    & --                     & --               & \makecell{89.86 / 72.51 \\(89.89 / 69.68)} \\ 
    SpERT (w/ RoBERTa-base)        & --                     & --               & \makecell{90.18 / 72.64 \\(90.30 / 72.18)} \\ 
    SpERT (w/ RoBERTa-base + LSTM) & --                     & --               & \makecell{89.17 / 75.03 \\(90.10 / 73.46)} \\ 
    \bottomrule
    \end{tabular}
    \caption{Results on CoNLL 2004. Pipeline results are reported in parentheses. $\dagger$ means that both training and development splits are used for training (see \href{https://github.com/lavis-nlp/spert/issues/2}{SpERT repo}).}
\end{table}


\subsubsection{SciERC}
\begin{table}[H]
    \centering
    \begin{tabular}{lccccc}
    \toprule
    Model & Paper & \makecell{Reported F1 \\(Ent / Rel)} & \makecell{Our Imp. F1 \\(Ent / Rel / Rel+)} & Notes \\
    \midrule
    SpERT (w/ CharLSTM + LSTM)     & --                     & --                & 59.63 / 34.25 / 23.04 & num\_layers=2 \\
    SpERT (w/ BERT-base)           & \citet{eberts2019span} & 67.62$^\dagger$ / 46.44$^\dagger$ & 66.71 / 46.07 / 33.94 \\
    SpERT (w/ BERT-base + LSTM)    & --                     & --                & 67.47 / 45.82 / 33.67 \\
    SpERT (w/ RoBERTa-base)        & --                     & --                & 69.29 / 48.93 / 36.65 \\
    SpERT (w/ RoBERTa-base + LSTM) & --                     & --                & 68.89 / 47.52 / 34.65 \\
    \bottomrule
    \end{tabular}
    \caption{Results on SciERC. $\dagger$ means that both training and development splits are used for training (see \href{https://github.com/lavis-nlp/spert/issues/2}{SpERT repo}).}
\end{table}


\newpage
\bibliographystyle{plainnat}
\bibliography{references} 

\end{document}
